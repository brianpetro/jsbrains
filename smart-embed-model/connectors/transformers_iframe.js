export const transformers_connector = "var __defProp = Object.defineProperty;\nvar __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;\nvar __publicField = (obj, key, value) => __defNormalProp(obj, typeof key !== \"symbol\" ? key + \"\" : key, value);\n\n// ../smart-model/adapters/_adapter.js\nvar SmartModelAdapter = class {\n  /**\n   * Create a SmartModelAdapter instance.\n   * @param {SmartModel} model - The parent SmartModel instance\n   */\n  constructor(model2) {\n    this.model = model2;\n    this.state = \"unloaded\";\n  }\n  /**\n   * Load the adapter.\n   * @async\n   * @returns {Promise<void>}\n   */\n  async load() {\n    this.set_state(\"loaded\");\n  }\n  /**\n   * Unload the adapter.\n   * @returns {void}\n   */\n  unload() {\n    this.set_state(\"unloaded\");\n  }\n  /**\n   * Get all settings.\n   * @returns {Object} All settings\n   */\n  get settings() {\n    return this.model.settings;\n  }\n  /**\n   * Get the current model key.\n   * @returns {string} Current model identifier\n   */\n  get model_key() {\n    return this.model.model_key;\n  }\n  /**\n   * Get the models.\n   * @returns {Object} Map of model objects\n   */\n  get models() {\n    const models = this.model.data.provider_models;\n    if (typeof models === \"object\" && Object.keys(models || {}).length > 0) return models;\n    else {\n      return {};\n    }\n  }\n  /**\n   * Get available models from the API.\n   * @abstract\n   * @param {boolean} [refresh=false] - Whether to refresh cached models\n   * @returns {Promise<Object>} Map of model objects\n   */\n  async get_models(refresh = false) {\n    throw new Error(\"get_models not implemented\");\n  }\n  /**\n   * Get available models as dropdown options synchronously.\n   * @returns {Array<Object>} Array of model options.\n   */\n  get_models_as_options() {\n    const models = this.models;\n    if (!Object.keys(models || {}).length) {\n      this.get_models(true);\n      return [{ value: \"\", name: \"No models currently available\" }];\n    }\n    return Object.entries(models).map(([id, model2]) => ({ value: id, name: model2.name || id })).sort((a, b) => a.name.localeCompare(b.name));\n  }\n  /**\n   * Set the adapter's state.\n   * @deprecated should be handled in SmartModel (only handle once)\n   * @param {('unloaded'|'loading'|'loaded'|'unloading')} new_state - The new state\n   * @throws {Error} If the state is invalid\n   */\n  set_state(new_state) {\n    const valid_states = [\"unloaded\", \"loading\", \"loaded\", \"unloading\"];\n    if (!valid_states.includes(new_state)) {\n      throw new Error(`Invalid state: ${new_state}`);\n    }\n    this.state = new_state;\n  }\n  // Replace individual state getters/setters with a unified state management\n  get is_loading() {\n    return this.state === \"loading\";\n  }\n  get is_loaded() {\n    return this.state === \"loaded\";\n  }\n  get is_unloading() {\n    return this.state === \"unloading\";\n  }\n  get is_unloaded() {\n    return this.state === \"unloaded\";\n  }\n};\n\n// adapters/_adapter.js\nvar SmartEmbedAdapter = class extends SmartModelAdapter {\n  /**\n   * Count tokens in input text\n   * @abstract\n   * @param {string} input - Text to tokenize\n   * @returns {Promise<Object>} Token count result\n   * @property {number} tokens - Number of tokens in input\n   * @throws {Error} If not implemented by subclass\n   */\n  async count_tokens(input) {\n    throw new Error(\"count_tokens method not implemented\");\n  }\n  /**\n   * Generate embeddings for single input\n   * @abstract\n   * @param {string|Object} input - Text to embed\n   * @returns {Promise<Object>} Embedding result\n   * @property {number[]} vec - Embedding vector\n   * @property {number} tokens - Number of tokens in input\n   * @throws {Error} If not implemented by subclass\n   */\n  async embed(input) {\n    if (typeof input === \"string\") input = { embed_input: input };\n    return (await this.embed_batch([input]))[0];\n  }\n  /**\n   * Generate embeddings for multiple inputs\n   * @abstract\n   * @param {Array<string|Object>} inputs - Texts to embed\n   * @returns {Promise<Array<Object>>} Array of embedding results\n   * @property {number[]} vec - Embedding vector for each input\n   * @property {number} tokens - Number of tokens in each input\n   * @throws {Error} If not implemented by subclass\n   */\n  async embed_batch(inputs) {\n    throw new Error(\"embed_batch method not implemented\");\n  }\n  get settings_config() {\n    return {\n      \"[ADAPTER].model_key\": {\n        name: \"Embedding model\",\n        type: \"dropdown\",\n        description: \"Select an embedding model.\",\n        options_callback: \"adapter.get_models_as_options\",\n        callback: \"model_changed\",\n        default: this.constructor.defaults.default_model\n      }\n    };\n  }\n  get dims() {\n    return this.model.data.dims;\n  }\n  get max_tokens() {\n    return this.model.data.max_tokens;\n  }\n  get batch_size() {\n    return this.model.data.batch_size || 1;\n  }\n};\n/**\n * @override in sub-class with adapter-specific default configurations\n * @property {string} id - The adapter identifier\n * @property {string} description - Human-readable description\n * @property {string} type - Adapter type (\"API\")\n * @property {string} endpoint - API endpoint\n * @property {string} adapter - Adapter identifier\n * @property {string} default_model - Default model to use\n */\n__publicField(SmartEmbedAdapter, \"defaults\", {});\n\n// adapters/transformers.js\nvar transformers_defaults = {\n  adapter: \"transformers\",\n  description: \"Transformers (Local, built-in)\",\n  default_model: \"TaylorAI/bge-micro-v2\",\n  models: transformers_models\n};\nvar DEVICE_CONFIGS = {\n  // // WebGPU: high quality first\n  webgpu_fp16: {\n    device: \"webgpu\",\n    dtype: \"fp16\",\n    quantized: false\n  },\n  webgpu_fp32: {\n    device: \"webgpu\",\n    dtype: \"fp32\",\n    quantized: false\n  },\n  // WebGPU: quantized tiers\n  webgpu_q8: {\n    device: \"webgpu\",\n    dtype: \"q8\",\n    quantized: true\n  },\n  webgpu_q4: {\n    device: \"webgpu\",\n    dtype: \"q4\",\n    quantized: true\n  },\n  // Optional, if you use it\n  webgpu_q4f16: {\n    device: \"webgpu\",\n    dtype: \"q4f16\",\n    quantized: true\n  },\n  webgpu_bnb4: {\n    device: \"webgpu\",\n    dtype: \"bnb4\",\n    quantized: true\n  },\n  // WASM: quantized CPU\n  wasm_q8: {\n    dtype: \"q8\",\n    quantized: true\n  },\n  wasm_q4: {\n    dtype: \"q4\",\n    quantized: true\n  },\n  // Final universal fallback: WASM CPU, dtype = auto\n  wasm_auto: {\n    // NOTE: leaving out device to avoid Linux issues with 'wasm'\n    // transformers.js will pick CPU/WASM backend itself\n    quantized: false\n  }\n};\nvar is_webgpu_available = async () => {\n  if (!(\"gpu\" in navigator)) return false;\n  const adapter = await navigator.gpu.requestAdapter();\n  if (!adapter) return false;\n  return true;\n};\nvar SmartEmbedTransformersAdapter = class extends SmartEmbedAdapter {\n  /**\n   * @param {import(\"../smart_embed_model.js\").SmartEmbedModel} model\n   */\n  constructor(model2) {\n    super(model2);\n    this.pipeline = null;\n    this.tokenizer = null;\n    this.active_config_key = null;\n    this.has_gpu = false;\n  }\n  /**\n   * Load the underlying transformers pipeline with WebGPU → WASM fallback.\n   * @returns {Promise<void>}\n   */\n  async load() {\n    this.has_gpu = await is_webgpu_available();\n    try {\n      if (this.loading) {\n        console.warn(\"[Transformers v2] load already in progress, waiting...\");\n        while (this.loading) {\n          await new Promise((resolve) => setTimeout(resolve, 100));\n        }\n      } else {\n        this.loading = true;\n        if (this.pipeline) {\n          this.loaded = true;\n          this.loading = false;\n          return;\n        }\n        await this.load_transformers_with_fallback();\n        this.loading = false;\n        this.loaded = true;\n        console.log(`[Transformers v2] model loaded using ${this.active_config_key}`, this);\n      }\n    } catch (e) {\n      this.loading = false;\n      this.loaded = false;\n      console.error(\"[Transformers v2] load failed\", e);\n      throw e;\n    }\n  }\n  /**\n   * Unload the pipeline and free resources.\n   * @returns {Promise<void>}\n   */\n  async unload() {\n    try {\n      if (this.pipeline) {\n        if (typeof this.pipeline.destroy === \"function\") {\n          this.pipeline.destroy();\n        } else if (typeof this.pipeline.dispose === \"function\") {\n          this.pipeline.dispose();\n        }\n      }\n    } catch (err) {\n      console.warn(\"[Transformers v2] error while disposing pipeline\", err);\n    }\n    this.pipeline = null;\n    this.tokenizer = null;\n    this.active_config_key = null;\n    this.loaded = false;\n  }\n  /**\n   * Available models – reuses the v1 transformers model catalog.\n   * @returns {Object}\n   */\n  get models() {\n    return transformers_models;\n  }\n  /**\n   * Maximum tokens per input.\n   * @returns {number}\n   */\n  get max_tokens() {\n    return this.model.data.max_tokens || 512;\n  }\n  /**\n   * Effective batch size.\n   * Prefers small deterministic batches when not explicitly configured.\n   * @returns {number}\n   */\n  get batch_size() {\n    const configured = this.model.data.batch_size;\n    if (configured && configured > 0) return configured;\n    return this.gpu_enabled ? 16 : 8;\n  }\n  get gpu_enabled() {\n    if (this.has_gpu) {\n      const explicit = typeof this.model.data.use_gpu === \"boolean\" ? this.model.data.use_gpu : null;\n      if (explicit === false) return false;\n      return true;\n    } else {\n      return false;\n    }\n  }\n  /**\n   * Initialize transformers pipeline with WebGPU → WASM fallback.\n   * @private\n   * @returns {Promise<void>}\n   */\n  async load_transformers_with_fallback() {\n    const { pipeline, env, AutoTokenizer } = await import(\"@huggingface/transformers\");\n    env.allowLocalModels = false;\n    if (typeof env.useBrowserCache !== \"undefined\") {\n      env.useBrowserCache = true;\n    }\n    let last_error = null;\n    const CONFIG_LIST_ORDER = Object.keys(DEVICE_CONFIGS);\n    const try_create = async (config_key) => {\n      const pipe = await pipeline(\"feature-extraction\", this.model_key, DEVICE_CONFIGS[config_key]);\n      return pipe;\n    };\n    for (const config of CONFIG_LIST_ORDER) {\n      if (this.pipeline) break;\n      if (config.includes(\"gpu\") && !this.gpu_enabled) {\n        console.warn(`[Transformers v2: ${config}] skipping ${config} as GPU is disabled`);\n        continue;\n      }\n      try {\n        console.log(`[Transformers v2] trying to load pipeline on ${config}`);\n        this.pipeline = await try_create(config);\n        this.active_config_key = config;\n        break;\n      } catch (err) {\n        console.warn(`[Transformers v2: ${config}] failed to load pipeline on ${config}`, err);\n        last_error = err;\n      }\n    }\n    if (this.pipeline) {\n      console.log(`[Transformers v2: ${this.active_config_key}] pipeline initialized using ${this.active_config_key}`);\n    } else {\n      throw last_error || new Error(\"Failed to initialize transformers pipeline\");\n    }\n    this.tokenizer = await AutoTokenizer.from_pretrained(this.model_key);\n  }\n  /**\n   * Count tokens in input text.\n   * @param {string} input\n   * @returns {Promise<{tokens:number}>}\n   */\n  async count_tokens(input) {\n    if (!this.tokenizer) {\n      await this.load();\n    }\n    const { input_ids } = await this.tokenizer(input);\n    return { tokens: input_ids.data.length };\n  }\n  /**\n   * Generate embeddings for multiple inputs.\n   * @param {Array<Object>} inputs\n   * @returns {Promise<Array<Object>>}\n   */\n  async embed_batch(inputs) {\n    if (!this.pipeline) {\n      await this.load();\n    }\n    const filtered_inputs = inputs.filter((item) => item.embed_input && item.embed_input.length > 0);\n    if (!filtered_inputs.length) return [];\n    const results = [];\n    for (let i = 0; i < filtered_inputs.length; i += this.batch_size) {\n      const batch = filtered_inputs.slice(i, i + this.batch_size);\n      const batch_results = await this._process_batch(batch);\n      results.push(...batch_results);\n    }\n    return results;\n  }\n  /**\n   * Process a single batch – with per-item retry on failure.\n   * @private\n   * @param {Array<Object>} batch_inputs\n   * @returns {Promise<Array<Object>>}\n   */\n  async _process_batch(batch_inputs) {\n    const prepared = await Promise.all(\n      batch_inputs.map((item) => this._prepare_input(item.embed_input))\n    );\n    const embed_inputs = prepared.map((p) => p.text);\n    const tokens = prepared.map((p) => p.tokens);\n    try {\n      const resp = await this.pipeline(embed_inputs, { pooling: \"mean\", normalize: true });\n      return batch_inputs.map((item, i) => {\n        const vec = Array.from(resp[i].data).map((val) => Math.round(val * 1e8) / 1e8);\n        item.vec = vec;\n        item.tokens = tokens[i];\n        return item;\n      });\n    } catch (err) {\n      console.error(\"[Transformers v2] batch embed failed \\u2013 retrying items individually\", err);\n      return await this._retry_items_individually(batch_inputs);\n    }\n  }\n  /**\n   * Prepare a single input by truncating to max_tokens if necessary.\n   * @private\n   * @param {string} embed_input\n   * @returns {Promise<{text:string,tokens:number}>}\n   */\n  async _prepare_input(embed_input) {\n    let { tokens } = await this.count_tokens(embed_input);\n    if (tokens <= this.max_tokens) {\n      return { text: embed_input, tokens };\n    }\n    let truncated = embed_input;\n    while (tokens > this.max_tokens && truncated.length > 0) {\n      const pct = this.max_tokens / tokens;\n      const max_chars = Math.floor(truncated.length * pct * 0.9);\n      truncated = truncated.slice(0, max_chars);\n      const last_space = truncated.lastIndexOf(\" \");\n      if (last_space > 0) {\n        truncated = truncated.slice(0, last_space);\n      }\n      tokens = (await this.count_tokens(truncated)).tokens;\n    }\n    return { text: truncated, tokens };\n  }\n  /**\n   * Retry each item individually after a batch failure.\n   * @private\n   * @param {Array<Object>} batch_inputs\n   * @returns {Promise<Array<Object>>}\n   */\n  async _retry_items_individually(batch_inputs) {\n    await this._reset_pipeline_after_error();\n    const results = [];\n    for (const item of batch_inputs) {\n      try {\n        const prepared = await this._prepare_input(item.embed_input);\n        const resp = await this.pipeline(prepared.text, { pooling: \"mean\", normalize: true });\n        const vec = Array.from(resp[0].data).map((val) => Math.round(val * 1e8) / 1e8);\n        results.push({\n          ...item,\n          vec,\n          tokens: prepared.tokens\n        });\n      } catch (single_err) {\n        console.error(\"[Transformers v2] single item embed failed \\u2013 skipping\", single_err);\n        results.push({\n          ...item,\n          vec: [],\n          tokens: 0,\n          error: single_err.message\n        });\n      }\n    }\n    return results;\n  }\n  /**\n   * Reset pipeline after a failure – falling back to WASM if needed.\n   * @private\n   * @returns {Promise<void>}\n   */\n  async _reset_pipeline_after_error() {\n    try {\n      if (this.pipeline) {\n        if (typeof this.pipeline.destroy === \"function\") {\n          this.pipeline.destroy();\n        } else if (typeof this.pipeline.dispose === \"function\") {\n          this.pipeline.dispose();\n        }\n      }\n    } catch (err) {\n      console.warn(\"[Transformers v2] error while resetting pipeline\", err);\n    }\n    this.pipeline = null;\n    await this.load_transformers_with_fallback();\n  }\n  /**\n   * V2 intentionally exposes only model selection in the settings UI.\n   * @returns {Object}\n   */\n  get settings_config() {\n    return super.settings_config;\n  }\n};\n__publicField(SmartEmbedTransformersAdapter, \"defaults\", transformers_defaults);\nvar transformers_models = {\n  \"TaylorAI/bge-micro-v2\": {\n    \"id\": \"TaylorAI/bge-micro-v2\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 512,\n    \"name\": \"BGE-micro-v2\",\n    \"description\": \"Local, 512 tokens, 384 dim (recommended)\",\n    \"adapter\": \"transformers\"\n  },\n  \"Snowflake/snowflake-arctic-embed-xs\": {\n    \"id\": \"Snowflake/snowflake-arctic-embed-xs\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 512,\n    \"name\": \"Snowflake Arctic Embed XS\",\n    \"description\": \"Local, 512 tokens, 384 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"Snowflake/snowflake-arctic-embed-s\": {\n    \"id\": \"Snowflake/snowflake-arctic-embed-s\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 512,\n    \"name\": \"Snowflake Arctic Embed Small\",\n    \"description\": \"Local, 512 tokens, 384 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"Snowflake/snowflake-arctic-embed-m\": {\n    \"id\": \"Snowflake/snowflake-arctic-embed-m\",\n    \"batch_size\": 1,\n    \"dims\": 768,\n    \"max_tokens\": 512,\n    \"name\": \"Snowflake Arctic Embed Medium\",\n    \"description\": \"Local, 512 tokens, 768 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"TaylorAI/gte-tiny\": {\n    \"id\": \"TaylorAI/gte-tiny\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 512,\n    \"name\": \"GTE-tiny\",\n    \"description\": \"Local, 512 tokens, 384 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"onnx-community/embeddinggemma-300m-ONNX\": {\n    \"id\": \"onnx-community/embeddinggemma-300m-ONNX\",\n    \"batch_size\": 1,\n    \"dims\": 768,\n    \"max_tokens\": 2048,\n    \"name\": \"EmbeddingGemma-300M\",\n    \"description\": \"Local, 2,048 tokens, 768 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"Mihaiii/Ivysaur\": {\n    \"id\": \"Mihaiii/Ivysaur\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 512,\n    \"name\": \"Ivysaur\",\n    \"description\": \"Local, 512 tokens, 384 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"andersonbcdefg/bge-small-4096\": {\n    \"id\": \"andersonbcdefg/bge-small-4096\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 4096,\n    \"name\": \"BGE-small-4K\",\n    \"description\": \"Local, 4,096 tokens, 384 dim\",\n    \"adapter\": \"transformers\"\n  },\n  // Too slow and persistent crashes\n  // \"jinaai/jina-embeddings-v2-base-de\": {\n  //   \"id\": \"jinaai/jina-embeddings-v2-base-de\",\n  //   \"batch_size\": 1,\n  //   \"dims\": 768,\n  //   \"max_tokens\": 4096,\n  //   \"name\": \"jina-embeddings-v2-base-de\",\n  //   \"description\": \"Local, 4,096 tokens, 768 dim, German\",\n  //   \"adapter\": \"transformers\"\n  // },\n  \"Xenova/jina-embeddings-v2-base-zh\": {\n    \"id\": \"Xenova/jina-embeddings-v2-base-zh\",\n    \"batch_size\": 1,\n    \"dims\": 768,\n    \"max_tokens\": 8192,\n    \"name\": \"Jina-v2-base-zh-8K\",\n    \"description\": \"Local, 8,192 tokens, 768 dim, Chinese/English bilingual\",\n    \"adapter\": \"transformers\"\n  },\n  \"Xenova/jina-embeddings-v2-small-en\": {\n    \"id\": \"Xenova/jina-embeddings-v2-small-en\",\n    \"batch_size\": 1,\n    \"dims\": 512,\n    \"max_tokens\": 8192,\n    \"name\": \"Jina-v2-small-en\",\n    \"description\": \"Local, 8,192 tokens, 512 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"nomic-ai/nomic-embed-text-v1.5\": {\n    \"id\": \"nomic-ai/nomic-embed-text-v1.5\",\n    \"batch_size\": 1,\n    \"dims\": 768,\n    \"max_tokens\": 2048,\n    \"name\": \"Nomic-embed-text-v1.5\",\n    \"description\": \"Local, 8,192 tokens, 768 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"Xenova/bge-small-en-v1.5\": {\n    \"id\": \"Xenova/bge-small-en-v1.5\",\n    \"batch_size\": 1,\n    \"dims\": 384,\n    \"max_tokens\": 512,\n    \"name\": \"BGE-small\",\n    \"description\": \"Local, 512 tokens, 384 dim\",\n    \"adapter\": \"transformers\"\n  },\n  \"nomic-ai/nomic-embed-text-v1\": {\n    \"id\": \"nomic-ai/nomic-embed-text-v1\",\n    \"batch_size\": 1,\n    \"dims\": 768,\n    \"max_tokens\": 2048,\n    \"name\": \"Nomic-embed-text\",\n    \"description\": \"Local, 2,048 tokens, 768 dim\",\n    \"adapter\": \"transformers\"\n  }\n};\n\n// build/transformers_iframe_script.js\nvar model = null;\nasync function process_message(data) {\n  const { method, params, id, iframe_id } = data;\n  try {\n    let result;\n    switch (method) {\n      case \"init\":\n        console.log(\"init\");\n        break;\n      case \"load\":\n        const model_params = { data: params, ...params };\n        console.log(\"load\", { model_params });\n        model = new SmartEmbedTransformersAdapter(model_params);\n        await model.load();\n        result = { model_loaded: true, model_config_key: model.active_config_key };\n        break;\n      case \"embed_batch\":\n        if (!model) throw new Error(\"Model not loaded\");\n        result = await model.embed_batch(params.inputs);\n        break;\n      case \"count_tokens\":\n        if (!model) throw new Error(\"Model not loaded\");\n        result = await model.count_tokens(params);\n        break;\n      default:\n        throw new Error(`Unknown method: ${method}`);\n    }\n    return { id, result, iframe_id };\n  } catch (error) {\n    console.error(\"Error processing message:\", error);\n    return { id, error: error.message, iframe_id };\n  }\n}\nprocess_message({ method: \"init\" });\n";