export const transformers_connector = "// models.json\nvar models_default = {\n  \"cohere-rerank-english-v3.0\": {\n    adapter: \"cohere\",\n    model_name: \"rerank-english-v3.0\",\n    model_description: \"Cohere Rerank English v3.0\",\n    model_version: \"3.0\",\n    endpoint: \"https://api.cohere.ai/v1/rerank\"\n  },\n  \"jinaai/jina-reranker-v1-tiny-en\": {\n    adapter: \"transformers\",\n    model_key: \"jinaai/jina-reranker-v1-tiny-en\"\n  },\n  \"jinaai/jina-reranker-v1-turbo-en\": {\n    adapter: \"transformers\",\n    model_key: \"jinaai/jina-reranker-v1-turbo-en\"\n  },\n  \"mixedbread-ai/mxbai-rerank-xsmall-v1\": {\n    adapter: \"transformers\",\n    model_key: \"mixedbread-ai/mxbai-rerank-xsmall-v1\"\n  },\n  \"Xenova/bge-reranker-base\": {\n    adapter: \"transformers\",\n    model_key: \"Xenova/bge-reranker-base\"\n  }\n};\n\n// smart_rank_model.js\nvar SmartRankModel = class _SmartRankModel {\n  /**\n   * Create a SmartRank instance.\n   * @param {string} env - The environment to use.\n   * @param {object} opts - Full model configuration object or at least a model_key and adapter\n   */\n  constructor(env, opts = {}) {\n    this.env = env;\n    this.opts = {\n      ...models_default[opts.model_key] || {},\n      ...opts\n    };\n    if (!this.opts.adapter) return console.warn(\"SmartRankModel adapter not set\");\n    if (!this.env.opts.smart_rank_adapters[this.opts.adapter]) return console.warn(`SmartRankModel adapter ${this.opts.adapter} not found`);\n    if (typeof navigator !== \"undefined\") this.opts.use_gpu = !!navigator?.gpu && this.opts.gpu_batch_size !== 0;\n    this.opts.use_gpu = false;\n    this.adapter = new this.env.opts.smart_rank_adapters[this.opts.adapter](this);\n  }\n  /**\n   * Used to load a model with a given configuration.\n   * @param {*} env \n   * @param {*} opts \n   */\n  static async load(env, opts = {}) {\n    if (env.smart_rank_active_models?.[opts.model_key]) return env.smart_rank_active_models[opts.model_key];\n    try {\n      const model2 = new _SmartRankModel(env, opts);\n      await model2.adapter.load();\n      if (!env.smart_rank_active_models) env.smart_rank_active_models = {};\n      env.smart_rank_active_models[opts.model_key] = model2;\n      return model2;\n    } catch (error) {\n      console.error(`Error loading rank model ${opts.model_key}:`, error);\n      return null;\n    }\n  }\n  async rank(query, documents) {\n    return this.adapter.rank(query, documents);\n  }\n};\n\n// adapters/_adapter.js\nvar SmartRankAdapter = class {\n  constructor(smart_rank) {\n    this.smart_rank = smart_rank;\n  }\n  async load() {\n    throw new Error(\"Not implemented\");\n  }\n  async rank(query, documents) {\n    throw new Error(\"Not implemented\");\n  }\n};\n\n// adapters/transformers.js\nvar SmartRankTransformersAdapter = class extends SmartRankAdapter {\n  constructor(smart_rank) {\n    super(smart_rank);\n    this.model = null;\n    this.tokenizer = null;\n  }\n  get use_gpu() {\n    return this.smart_rank.opts.use_gpu || false;\n  }\n  async load() {\n    console.log(\"TransformersAdapter initializing\");\n    const { env, AutoTokenizer, AutoModelForSequenceClassification } = await import(\"https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.0\");\n    console.log(\"Transformers loaded\");\n    env.allowLocalModels = false;\n    const pipeline_opts = {\n      quantized: true\n    };\n    if (this.use_gpu) {\n      console.log(\"[Transformers] Using GPU\");\n      pipeline_opts.device = \"webgpu\";\n      pipeline_opts.dtype = \"fp32\";\n    } else {\n      console.log(\"[Transformers] Using CPU\");\n      env.backends.onnx.wasm.numThreads = 8;\n    }\n    this.model = await AutoModelForSequenceClassification.from_pretrained(this.smart_rank.opts.model_key, pipeline_opts);\n    console.log(\"Model loaded\");\n    this.tokenizer = await AutoTokenizer.from_pretrained(this.smart_rank.opts.model_key);\n    console.log(\"Tokenizer loaded\");\n    console.log(\"TransformersAdapter initialized\");\n  }\n  async rank(query, documents, options = {}) {\n    const { top_k = void 0, return_documents = false } = options;\n    const inputs = this.tokenizer(\n      new Array(documents.length).fill(query),\n      { text_pair: documents, padding: true, truncation: true }\n    );\n    const { logits } = await this.model(inputs);\n    return logits.sigmoid().tolist().map(([score], i) => ({\n      index: i,\n      score,\n      ...return_documents ? { text: documents[i] } : {}\n    })).sort((a, b) => b.score - a.score).slice(0, top_k);\n  }\n};\n\n// build/transformers_iframe_script.js\nvar model = null;\nvar smart_env = {\n  smart_rank_active_models: {},\n  opts: {\n    smart_rank_adapters: {\n      transformers: SmartRankTransformersAdapter\n    }\n  }\n};\nasync function processMessage(data) {\n  const { method, params, id, iframe_id } = data;\n  try {\n    let result;\n    switch (method) {\n      case \"init\":\n        console.log(\"init\");\n        break;\n      case \"load\":\n        console.log(\"load\", params);\n        model = await SmartRankModel.load(smart_env, { adapter: \"transformers\", model_key: params.model_key, ...params });\n        result = { model_loaded: true };\n        break;\n      case \"rank\":\n        if (!model) throw new Error(\"Model not loaded\");\n        result = await model.rank(params.query, params.documents);\n        break;\n      default:\n        throw new Error(`Unknown method: ${method}`);\n    }\n    return { id, result, iframe_id };\n  } catch (error) {\n    console.error(\"Error processing message:\", error);\n    return { id, error: error.message, iframe_id };\n  }\n}\nprocessMessage({ method: \"init\" });\n";